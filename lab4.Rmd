---
title: "R Notebook"
output: html_notebook
---

# Lab assignment 4

## Team

*Vladyslav Vasylchenko, Max Golovin, Ostap Kostiuk*

## Work distribution

Vlad - 1, 2\
Max - 3\
Ostap - 4

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Problem 1

In this problem we have two independent samples $X_1,\dots,X_{100}$ and
$Y_1,\dots,Y_{50}$, which we model as observations from normal
distributions $X_i \sim N(\mu_1, 1)$ and $Y_j \sim N(\mu_2, 1)$.

We test the hypotheses
$H_0: \mu_1 = \mu_2 \quad \text{vs} \quad H_1: \mu_1 \neq \mu_2$ under
known variances $\sigma_1^2 = \sigma_2^2 = 1$.

The sequence $a_k$ is defined as $$
a_k = \{ k \ln(k^2 n + \pi) \}, \quad k \ge 1,
$$ where the fractional part of a number $x$ is defined as $$
\{x\} = x - \lfloor x \rfloor .
$$

The data are then generated as $$
x_k = \Phi^{-1}(a_k), \quad k = 1,\dots,100,
$$ $$
y_l = \Phi^{-1}(a_{l+100}), \quad l = 1,\dots,50,
$$ where $\Phi^{-1}$ is the quantile function of the standard normal
distribution (the `qnorm` function in R).

Since the variances are known, the standard procedure is the two-sample
Z-test for the difference of means.

```{r}
team_id <- 10

a_fun <- function(k, n) {
  x <- k * log(k^2 * n + pi)
  x - floor(x)
}

k_max  <- 150
a_data <- sapply(1:k_max, a_fun, n = team_id)

range(a_data)
```

```{r}
x <- qnorm(a_data[1:100])

y <- qnorm(a_data[101:150])

length(x); length(y)
```

We denote the sample means as $$
\bar X = \frac{1}{n_1}\sum_{i=1}^{n_1} X_i,
\qquad
\bar Y = \frac{1}{n_2}\sum_{j=1}^{n_2} Y_j,
$$ where $n_1 = 100$ and $n_2 = 50$.

According to the problem assumptions, $$
X_i \sim N(\mu_1, 1), \qquad Y_j \sim N(\mu_2, 1),
$$ therefore $$
\bar X \sim N\!\left(\mu_1,\; \frac{1}{n_1}\right), 
\qquad
\bar Y \sim N\!\left(\mu_2,\; \frac{1}{n_2}\right).
$$

Then the difference of the sample means has the distribution $$
\bar X - \bar Y \sim
N\!\left(\mu_1 - \mu_2,\; \frac{1}{n_1} + \frac{1}{n_2}\right).
$$

Under the null hypothesis $H_0:\mu_1 = \mu_2$ we have $$
\bar X - \bar Y \sim
N\!\left(0,\; \frac{1}{n_1} + \frac{1}{n_2}\right).
$$

Hence, the test statistic $$
Z = \frac{\bar X - \bar Y}
         {\sqrt{\frac{1}{n_1} + \frac{1}{n_2}}}
$$ has the standard normal distribution $N(0,1)$ when $H_0$ is true.

```{r}
n1 <- length(x)
n2 <- length(y)

x_bar <- mean(x)
y_bar <- mean(y)

n1; n2
x_bar; y_bar
```

```{r}
sigma1_sq <- 1
sigma2_sq <- 1

se_diff <- sqrt(sigma1_sq / n1 + sigma2_sq / n2)

z_obs <- (x_bar - y_bar) / se_diff
z_obs
```

We use a two-sided test because the alternative hypothesis is
$H_1:\mu_1 \neq \mu_2$.

The significance level is $\alpha = 0.05$.

For the standard normal distribution the critical points are
$z_{1-\alpha/2} = z_{0.975}$. The critical region is $$
\mathrm{Cr} = \{ z : |z| > z_{1-\alpha/2} \}.
$$

```{r}
alpha  <- 0.05
z_crit <- qnorm(1 - alpha/2)
z_crit

abs(z_obs) > z_crit
```

If $\lvert Z_{\text{obs}}\rvert > z_{1-\alpha/2}$, we reject $H_0$.

For the two-sided Z-test the p-value is computed as $$
p\text{-value}
= 2\bigl(1 - \Phi(\lvert Z_{\text{obs}}\rvert)\bigr),
$$ where $\Phi$ is the cumulative distribution function of the standard
normal distribution.

```{r}
p_value <- 2 * (1 - pnorm(abs(z_obs)))
p_value
```

Interpretation:

-   If $p\text{-value} < 0.05$, we reject $H_0$ at the significance
    level $\alpha = 0.05$ and conclude that $\mu_1$ and $\mu_2$ are
    statistically different.

-   If $p\text{-value} \ge 0.05$, we do not reject $H_0$: the observed
    data do not provide sufficient evidence to claim that the population
    means are different.

# Problem 2

In this part we test hypotheses about the variances of two normal
populations:

$$
H_0: \sigma_1^2 = \sigma_2^2 
\quad \text{vs} \quad 
H_1: \sigma_1^2 > \sigma_2^2,
$$

where $X_1,\dots,X_{n_1}$ and $Y_1,\dots,Y_{n_2}$ are the same samples
as in Problem 1, with $n_1 = 100$, $n_2 = 50$, and the means $\mu_1$,
$\mu_2$ are assumed to be unknown.

We assume that $$
X_i \sim N(\mu_1, \sigma_1^2), \qquad Y_j \sim N(\mu_2, \sigma_2^2),
$$ and that the samples are independent. The standard test for comparing
two variances of normal distributions is the **F-test**.

The sample variances are defined as $$
S_1^2 = \frac{1}{n_1-1} \sum_{i=1}^{n_1} (X_i - \bar X)^2, \qquad
S_2^2 = \frac{1}{n_2-1} \sum_{j=1}^{n_2} (Y_j - \bar Y)^2.
$$

For normal data we know that $$
\frac{(n_1-1)S_1^2}{\sigma_1^2} \sim \chi^2_{n_1-1},
\qquad
\frac{(n_2-1)S_2^2}{\sigma_2^2} \sim \chi^2_{n_2-1},
$$ and these two chi-squared variables are independent.

Under the null hypothesis $H_0:\sigma_1^2 = \sigma_2^2$ the test
statistic $$
F = \frac{S_1^2}{S_2^2}
$$ has the F-distribution with $(n_1-1, n_2-1)$ degrees of freedom: $$
F \sim F_{n_1-1,,n_2-1}.
$$

Since the alternative hypothesis is one-sided
($\sigma_1^2 > \sigma_2^2$), the critical region for significance level
$\alpha = 0.05$ is $$
\mathrm{Cr} = { f : f > F_{1-\alpha}(n_1-1, n_2-1) },
$$ where $F_{1-\alpha}(n_1-1, n_2-1)$ is the $(1-\alpha)$-quantile of
the $F_{n_1-1,,n_2-1}$ distribution.

```{r}
s1_sq <- var(x)
s2_sq <- var(y)

s1_sq
s2_sq

F_obs <- s1_sq / s2_sq
F_obs
```

```{r}
alpha <- 0.05

F_crit <- qf(1 - alpha, df1 = n1 - 1, df2 = n2 - 1)
F_crit

F_obs > F_crit
```

If $F_obs \> F_crit$, we reject $H_0$ at the significance level
$\alpha = 0.05$. Otherwise, we do not reject $H_0$.

For the one-sided test $H_1:\sigma_1^2 > \sigma_2^2$ the p-value is $$
p\text{-value}
= P\bigl(F_{n_1-1,,n_2-1} \ge F_{\text{obs}}\bigr)
= 1 - F_F(F_{\text{obs}}),
$$ where $F_F$ is the cumulative distribution function of the
F-distribution.

```{r}
p_value_F <- 1 - pf(F_obs, df1 = n1 - 1, df2 = n2 - 1)
p_value_F
```

Interpretation:

• If $p\text{-value} < 0.05$, we reject $H_0$ at the significance level
$\alpha = 0.05$ and conclude that the variance of the first population
is significantly larger than the variance of the second population.

• If $p\text{-value} \ge 0.05$, we do not reject $H_0$. In this case,
the observed data do not provide sufficient evidence to claim that
$\sigma_1^2 > \sigma_2^2$.

## Task 3
The problem asks to check distribution properties of the generated samples using the Kolmogorov–Smirnov (KS) test.\

The Kolmogorov–Smirnov test is a non-parametric test used to determine if a sample comes from a specific distribution (one-sample KS test) or if two samples come from the same distribution (two-sample KS test).\
The test is based on the Empirical Cumulative Distribution Function.\
One-Sample: The KS test statistic, $D$, measures the maximum absolute difference between the ECDF of the sample, $F_n(t)$, and the hypothesized theoretical Cumulative Distribution Function (CDF), $F(t)$.$$D = \sup_t |F_n(t) - F(t)|$$\
Two-Sample: The KS test statistic, $D$, measures the maximum absolute difference between the ECDFs of the two samples, $F_{n_1}(t)$ and $F_{n_2}(t)$.$$D = \sup_t |F_{n_1}(t) - F_{n_2}(t)|$$\
A large value of $D$ suggests a poor fit or different underlying distributions. The test rejects the null hypothesis ($H_0$) if the $D$ statistic exceeds a critical value $C$ (based on the sample size(s) and significance level $\alpha$), or, equivalently, if the $p$-value is less than $\alpha$.\
The function $a_k = \{k \ln (k^{2n} + \pi)\}$ generates a sequence in $(0, 1)$ that approximates a uniform $U(0, 1)$ distribution. Applying $\Phi^{-1}$ (the qnorm function) transforms this uniform sequence into a sequence that approximates a standard normal $N(0, 1)$ distribution.\

For a significance level $\alpha=0.05$, the rejection rule is:$$\text{Reject } H_0 \text{ if } D > C, \quad \text{or, equivalently, if } p\text{-value} < 0.05$$Where $D$ is the KS test statistic (the maximum absolute difference between the ECDFs) and $C$ is the critical value for the specified $\alpha$.\
```{r}
n <- 10
set.seed(n)
k_values <- 1:150

a_k_full <- (k_values * log(k_values^(2*n) + pi)) %% 1

a_x <- a_k_full[1:100]
a_y <- a_k_full[101:150]

x <- qnorm(a_x)
y <- qnorm(a_y)
```

a)\
Task: Check if $\{x_k\}_{k=1}^{100}$ are normally distributed (with parameters calculated from the sample).\
I used Kolmogorov–Smirnov test to compare an empirical distribution function (EDF) against a specified theoretical distribution function (CDF).\
Hypotheses\
$H_0$: $\{x_k\}$ follows a Normal distribution $N(\mu, \sigma^2)$ where $\mu$ and $\sigma$ are the sample mean and standard deviation.\
$H_1$: $\{x_k\}$ does not follow this Normal distribution.\

```{r}
sample_mean_x <- mean(x)
sample_sd_x <- sd(x)
cat("Sample Mean of x:", sample_mean_x, "\n")
cat("Sample SD of x:", sample_sd_x, "\n")
```

```{r}
ks_test_a <- ks.test(x, "pnorm", mean = sample_mean_x, sd = sample_sd_x)
print(ks_test_a)
```
Since our p-value(0.8402) is > 0.05, we can't reject $H_0$\
The generated $x_k$ values are derived directly from the standard normal CDF inverse ($\Phi^{-1}$) applied to a $U(0, 1)$ sequence. Thus, the data is by construction an excellent approximation of a standard normal distribution. The high $p$-value confirms that the observed deviations from the normal CDF are well within the range expected by chance if $H_0$ were true.\

b)
Task: Check if $\{|x_k|\}_{k=1}^{100}$ are exponentially distributed with $\lambda = 1$.\
I used Kolmogorov–Smirnov test to compare the EDF of the sample $\mathbf{|x|}$ against the fixed theoretical $\text{Exp}(\lambda=1)$ CDF.\
$H_0$: $\{|x_k|\}$ follows an $\text{Exp}(\lambda=1)$ distribution (mean $= 1$).\
$H_1$: $\{|x_k|\}$ does not follow this Exponential distribution.\

```{r}
abs_x <- abs(x)
sample_mean_abs_x <- mean(abs_x)
cat("Sample Mean of |x|:", sample_mean_abs_x, "\n")
```

```{r}
ks_test_b <- ks.test(abs_x, "pexp", rate = 1)
print(ks_test_b)
```
Since our p-value(0.09886) is > 0.05, we can't reject $H_0$\
The sample $\mathbf{|x|}$ comes from the absolute values of a standard normal distribution. We conclude there is insufficient evidence at the $\alpha=0.05$ level to say the data is not Exponential with $\lambda=1$.\

c)
Task: Check if $\{x_k\}_{k=1}^{100}$ and $\{y_l\}_{l=1}^{50}$ have the same distributions.\
I used Kolmogorov–Smirnov test to compare the EDFs of two independent samples ($\mathbf{x}$ and $\mathbf{y}$) to determine if they originate from the same underlying continuous distribution.\
$H_0$: $\{x_k\}$ and $\{y_l\}$ are drawn from the same continuous distribution.\
$H_1$: $\{x_k\}$ and $\{y_l\}$ are drawn from different continuous distributions.\

```{r}
sample_mean_y <- mean(y)
sample_sd_y <- sd(y)
cat("Sample Mean of x:", sample_mean_x, "\n")
cat("Sample Mean of y:", sample_mean_y, "\n")
cat("Sample SD of x:", sample_sd_x, "\n")
cat("Sample SD of y:", sample_sd_y, "\n")
```

```{r}
ks_test_c <- ks.test(x, y)
print(ks_test_c)
```
Since our p-value(0.4287) is > 0.05, we can't reject $H_0$\
Both samples $\mathbf{x}$ and $\mathbf{y}$ are generated using the exact same $\Phi^{-1}$ transformation applied to non-overlapping segments of the same underlying pseudo-random sequence $\{a_k\}$. Therefore, they are drawn from the same underlying theoretical distribution (Standard Normal). The two-sample KS test correctly concludes that the small differences between the samples are likely due to sampling variability and not a fundamental difference in their underlying distributions.\
## Task 4

### Problem Statement

...

### Solution

...
